{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "In this module, we will use the Natural Language Toolkit Library (NLTK) to look at individual words and sentences in a text and clean unneccessary features from the text data to prepare for sentiment analysis. Then using the textblob library, we will analyze the sentiment of opinioned data to give a numerical value for use in a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Words and Sentences\n",
    "\n",
    "Recall in the \"Python Dictionaries and String Manipulation\" notebook, we used the .split() function to break a sentence apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'color', 'is', 'purple']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My favorite color is purple\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because the default character to split on is a space, the .split() function does not work well with sentences that have punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'favorite',\n",
       " 'foods',\n",
       " 'are',\n",
       " 'french',\n",
       " 'fries,',\n",
       " 'bacon,',\n",
       " 'and',\n",
       " 'cheese']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the comma is attached to the previous word\n",
    "text2 = \"My favorite foods are french fries, bacon, and cheese\"\n",
    "text2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library was built to separate punctuation from words when tokenizing (splitting into parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GBTC408011ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GBTC408011ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\GBTC408011ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GBTC408011ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\GBTC408011ur\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#this is sample data\n",
    "from nltk.corpus import names  \n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#if the next cell does not work\n",
    "#remove number symbol on following lines and re-run this cell\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'favorite',\n",
       " 'foods',\n",
       " 'are',\n",
       " 'french',\n",
       " 'fries',\n",
       " ',',\n",
       " 'bacon',\n",
       " ',',\n",
       " 'and',\n",
       " 'cheese']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the comma is now its own token when the sentence is split\n",
    "word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Kenisha Priester.',\n",
       " 'My favorite color is purple.',\n",
       " 'My favorite foods are french fries, bacon, and cheese.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#muti-sentence texts can be tokenized by sentence\n",
    "#each sentence is an item in the list\n",
    "text3 = \"My name is Kenisha Priester. My favorite color is purple. My favorite foods are french fries, bacon, and cheese.\"\n",
    "\n",
    "sent_tokenize(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of social media data is in the form of tweets. A tweet can have an @ sign to tag another user or a # sign to tag a particular subject. When analyzing data, you may want to retain these symbols with the words/phrase they're attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', 'animegurl', 'OMG', 'you', 'are', 'so', '#', 'funny', ':', 'P']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets are more difficult to use the word tokenize function on\n",
    "#using word_tokenize, common social media signs get separated\n",
    "tweet = \"@animegurl OMG you are so #funny :P\"\n",
    "\n",
    "word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@animegurl', 'OMG', 'you', 'are', 'so', '#funny', ':P']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeps the @ sign, # sign, and the tongue-out emoji\n",
    "TweetTokenizer().tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember the Word Count exercise text?\n",
    "\n",
    "Let's clean it up using NLTK and do a basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#work count exercise paragraph\n",
    "wctext = '''\n",
    "A CerTaiN kING HaD a bEaUtIFul gaRDEn, ANd IN THe GarDen StooD A trEe\n",
    "whICH bORe GoLDeN ApPlEs. THesE aPples WerE alwAyS CoUntEd, aNd abOuT\n",
    "tHE TiMe WheN tHEY BEgAn tO grow RipE IT wAs foUND THat EVeRY NIgHT ONE\n",
    "OF THeM Was gOne. thE kiNg bECAMe veRy ANGRy at thiS, aND ORDEred the\n",
    "GarDEneR TO KEeP WAtch ALL NIgHT uNDER the tREE. tHE gardener sEt hIs\n",
    "ELdEsT SoN to WATCH; buT ABout TweLve O'clOCK He fELL ASlEEp, And in\n",
    "the morNIng aNOTheR Of thE aPPLes Was mIssinG. tHEn THE sECONd Son waS\n",
    "oRdERED to waTch; aNd AT mIDniGhT he tOO FELl ASleEP, aND iN thE mOrNIng\n",
    "ANoThER AppLE WaS gOne . TheN THe thIrd Son oFfeREd tO KeEp wATCh; buT\n",
    "thE garDENer At First WoULd NoT LET Him, FOr fEaR sOMe HArM ShOuLD cOME\n",
    "To hIM: hoWeveR, at lAST hE coNSEnteD, AND tHE YouNg MAN laID HimSELf\n",
    "uNDER tHE tREe TO wAtch. AS tHE clocK STRuCk tweLvE He Heard A rustlinG\n",
    "NoISe IN ThE aIr, And a biRd CAME FlYing ThAt was Of PUre gOLd; AND as\n",
    "IT WAs SNApPING At onE oF ThE aPpleS wIth iTS BeaK, tHE GArDEner’S son\n",
    "jUMpED UP And SHOT AN aRrOw at iT. But THE arrOw DID thE BiRD nO HaRm;\n",
    "ONlY iT dRoPPEd a GoLDEn FEather FROM iTS tAiL, aND THEN FLEw AwaY.\n",
    "the gOLdEN FEAthER WAS bRoUght to THe KinG IN THE MOrNING, AnD aLL ThE\n",
    "cOunCil WAs called TogETHER. EVERYoNE aGREed ThAt it wAS wORth MoRe THAn\n",
    "aLl The weAltH Of tHE kIngDOm: But THE KiNg sAID, ‘One FeatHeR Is Of NO\n",
    "use tO me, I MusT HaVE ThE wHOLE BIRD .’\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, change all the words to lowercase\n",
    "wctext = wctext.lower()\n",
    "\n",
    "#then tokenize each part of the text\n",
    "tknz_wct = word_tokenize(wctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'certain', 'king', 'had', 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz_wct[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tknz_wct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 14,\n",
       "          '.': 10,\n",
       "          ':': 2,\n",
       "          ';': 5,\n",
       "          'a': 6,\n",
       "          'about': 2,\n",
       "          'agreed': 1,\n",
       "          'air': 1,\n",
       "          'all': 3,\n",
       "          'always': 1,\n",
       "          'an': 1,\n",
       "          'and': 12,\n",
       "          'angry': 1,\n",
       "          'another': 2,\n",
       "          'apple': 1,\n",
       "          'apples': 4,\n",
       "          'arrow': 2,\n",
       "          'as': 2,\n",
       "          'asleep': 2,\n",
       "          'at': 6,\n",
       "          'away': 1,\n",
       "          'beak': 1,\n",
       "          'beautiful': 1,\n",
       "          'became': 1,\n",
       "          'began': 1,\n",
       "          'bird': 3,\n",
       "          'bore': 1,\n",
       "          'brought': 1,\n",
       "          'but': 4,\n",
       "          'called': 1,\n",
       "          'came': 1,\n",
       "          'certain': 1,\n",
       "          'clock': 1,\n",
       "          'come': 1,\n",
       "          'consented': 1,\n",
       "          'council': 1,\n",
       "          'counted': 1,\n",
       "          'did': 1,\n",
       "          'dropped': 1,\n",
       "          'eldest': 1,\n",
       "          'every': 1,\n",
       "          'everyone': 1,\n",
       "          'fear': 1,\n",
       "          'feather': 3,\n",
       "          'fell': 2,\n",
       "          'first': 1,\n",
       "          'flew': 1,\n",
       "          'flying': 1,\n",
       "          'for': 1,\n",
       "          'found': 1,\n",
       "          'from': 1,\n",
       "          'garden': 2,\n",
       "          'gardener': 4,\n",
       "          'gold': 1,\n",
       "          'golden': 3,\n",
       "          'gone': 2,\n",
       "          'grow': 1,\n",
       "          'had': 1,\n",
       "          'harm': 2,\n",
       "          'have': 1,\n",
       "          'he': 4,\n",
       "          'heard': 1,\n",
       "          'him': 2,\n",
       "          'himself': 1,\n",
       "          'his': 1,\n",
       "          'however': 1,\n",
       "          'i': 1,\n",
       "          'in': 5,\n",
       "          'is': 1,\n",
       "          'it': 5,\n",
       "          'its': 2,\n",
       "          'jumped': 1,\n",
       "          'keep': 2,\n",
       "          'king': 4,\n",
       "          'kingdom': 1,\n",
       "          'laid': 1,\n",
       "          'last': 1,\n",
       "          'let': 1,\n",
       "          'man': 1,\n",
       "          'me': 1,\n",
       "          'midnight': 1,\n",
       "          'missing': 1,\n",
       "          'more': 1,\n",
       "          'morning': 3,\n",
       "          'must': 1,\n",
       "          'night': 2,\n",
       "          'no': 2,\n",
       "          'noise': 1,\n",
       "          'not': 1,\n",
       "          \"o'clock\": 1,\n",
       "          'of': 6,\n",
       "          'offered': 1,\n",
       "          'one': 3,\n",
       "          'only': 1,\n",
       "          'ordered': 2,\n",
       "          'pure': 1,\n",
       "          'ripe': 1,\n",
       "          'rustling': 1,\n",
       "          's': 1,\n",
       "          'said': 1,\n",
       "          'second': 1,\n",
       "          'set': 1,\n",
       "          'shot': 1,\n",
       "          'should': 1,\n",
       "          'snapping': 1,\n",
       "          'some': 1,\n",
       "          'son': 4,\n",
       "          'stood': 1,\n",
       "          'struck': 1,\n",
       "          'tail': 1,\n",
       "          'than': 1,\n",
       "          'that': 3,\n",
       "          'the': 28,\n",
       "          'them': 1,\n",
       "          'then': 3,\n",
       "          'these': 1,\n",
       "          'they': 1,\n",
       "          'third': 1,\n",
       "          'this': 1,\n",
       "          'time': 1,\n",
       "          'to': 9,\n",
       "          'together': 1,\n",
       "          'too': 1,\n",
       "          'tree': 3,\n",
       "          'twelve': 2,\n",
       "          'under': 2,\n",
       "          'up': 1,\n",
       "          'use': 1,\n",
       "          'very': 1,\n",
       "          'was': 10,\n",
       "          'watch': 5,\n",
       "          'wealth': 1,\n",
       "          'were': 1,\n",
       "          'when': 1,\n",
       "          'which': 1,\n",
       "          'whole': 1,\n",
       "          'with': 1,\n",
       "          'worth': 1,\n",
       "          'would': 1,\n",
       "          'young': 1,\n",
       "          '‘': 1,\n",
       "          '’': 2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the NLTK FreqDist gives a count for how often each part of the text occurs\n",
    "fd_wct = FreqDist(tknz_wct)\n",
    "fd_wct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 28),\n",
       " (',', 14),\n",
       " ('and', 12),\n",
       " ('.', 10),\n",
       " ('was', 10),\n",
       " ('to', 9),\n",
       " ('a', 6),\n",
       " ('of', 6),\n",
       " ('at', 6),\n",
       " ('in', 5)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the top 10 words in the text\n",
    "fd_wct.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common parts of this text seem to be filler words and punctuation. We need to remove them to get a better understand of what the text is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of tokens in list before puntuation removal\n",
    "len(tknz_wct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the puntuation tokens from the list\n",
    "for word in tknz_wct:\n",
    "    if word in punctuation:\n",
    "        tknz_wct.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of tokens in list after puntuation removal\n",
    "len(tknz_wct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of english stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm_count = 0\n",
    "new_words = []  #list to hold new words\n",
    "\n",
    "for word in tknz_wct:\n",
    "    if word not in eng_stopwords:\n",
    "        new_words.append(word)\n",
    "    else: rm_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the new top 10 words in this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('watch', 5),\n",
       " ('king', 4),\n",
       " ('apples', 4),\n",
       " ('gardener', 4),\n",
       " ('son', 4),\n",
       " ('tree', 3),\n",
       " ('golden', 3),\n",
       " ('one', 3),\n",
       " ('morning', 3),\n",
       " ('bird', 3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_nw = FreqDist(new_words)\n",
    "fd_nw.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in Enlgish can change their form depending on if it's past tense, present tense, or future tense. We can reduce these words to their dictionary form so that it's easier for the computer to interpret the words. This is called **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put the word lemmatization function into a variable\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this sentence contains words with different tenses\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize the sentence into a list\n",
    "#this is before we lemmatize it\n",
    "non_lem = word_tokenize(sentence)\n",
    "non_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#empty list to hold the new lemmatized words\n",
    "lemm = []\n",
    "\n",
    "for word in non_lem:\n",
    "    lemm.append(wnl.lemmatize(word, pos=\"v\"))  #lemmatize using 'verb' part-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'be',\n",
       " 'run',\n",
       " 'and',\n",
       " 'eat',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " '.',\n",
       " 'He',\n",
       " 'have',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swim',\n",
       " 'after',\n",
       " 'play',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the list of tokens after being lemmatized\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual analysis of last letter of male and female names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are two text files within the Names sample data\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the words in the text to lists\n",
    "m_names = names.words('male.txt')\n",
    "f_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample the first 5 items in m_names\n",
    "m_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make a frequency distribution of names that end with a particular letter (by gender)\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (fileid, name[-1])\n",
    "            for fileid in names.fileids()\n",
    "            for name in names.words(fileid))\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "In order to understand how people feel about something, we need to do sentiment analysis on text data that contains their opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#initilize function to do sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myday = \"Today is a great day, but it is boring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.1027, 'neg': 0.257, 'neu': 0.522, 'pos': 0.222}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(myday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1027"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the sentiment value from the dictionary of scores\n",
    "sid.polarity_scores(myday)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd_comp = sid.polarity_scores(myday)['compound']\n",
    "type(vd_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a sentiment value column in a dataframe\n",
    "\n",
    "Using the [Amazon Book Reviews dataset on Kaggle](https://www.kaggle.com/shrutimehta/amazon-book-reviews-webscraped), we add a new column to the dataset that will have a numerical value for the sentiment of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good. It IS a page turner. You can read this b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are no words for how much I loathed this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think I would ordinarily cut this book more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three disjointed characters for whom it's hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was snookered into this novel as it was compar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ReviewContent\n",
       "0  Good. It IS a page turner. You can read this b...\n",
       "1  There are no words for how much I loathed this...\n",
       "2  I think I would ordinarily cut this book more ...\n",
       "3  Three disjointed characters for whom it's hard...\n",
       "4  Was snookered into this novel as it was compar..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load the data from the Reviews.csv file\n",
    "filepath = \"Reviews.csv\"\n",
    "df = pd.read_csv(filepath, encoding = \"latin-1\") #this file is encoded differently\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function to clean up each review\n",
    "#then it will analyze and assign a sentiment polarity\n",
    "def reviewSentiment(review):\n",
    "    \n",
    "    #make text lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    #tokenize the review\n",
    "    tknz_review = word_tokenize(review)\n",
    "    \n",
    "    #remove puntuation\n",
    "    for token in tknz_review:\n",
    "        if token in punctuation:\n",
    "            tknz_review.remove(token)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    #remove filler words\n",
    "    for token in tknz_review:\n",
    "        if token not in eng_stopwords:\n",
    "            clean_tokens.append(token)\n",
    "            \n",
    "    #put sentence back together with remaining clean words\n",
    "    clean_review = ' '.join(clean_tokens)\n",
    "    #clean_review = ' '.join(tknz_review)\n",
    "    \n",
    "    #turn into textblob\n",
    "    sid_rev = sid.polarity_scores(clean_review)\n",
    "    \n",
    "    #get sentiment polarity\n",
    "    r_comp = sid_rev['compound']\n",
    "    \n",
    "    return r_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a new column to hold sentiment value from function\n",
    "df['review_sentiment'] = df['ReviewContent'].apply(reviewSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewContent</th>\n",
       "      <th>review_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good. It IS a page turner. You can read this b...</td>\n",
       "      <td>-0.4086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are no words for how much I loathed this...</td>\n",
       "      <td>-0.7432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think I would ordinarily cut this book more ...</td>\n",
       "      <td>0.9743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three disjointed characters for whom it's hard...</td>\n",
       "      <td>0.5496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was snookered into this novel as it was compar...</td>\n",
       "      <td>0.7329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ReviewContent  review_sentiment\n",
       "0  Good. It IS a page turner. You can read this b...           -0.4086\n",
       "1  There are no words for how much I loathed this...           -0.7432\n",
       "2  I think I would ordinarily cut this book more ...            0.9743\n",
       "3  Three disjointed characters for whom it's hard...            0.5496\n",
       "4  Was snookered into this novel as it was compar...            0.7329"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#erify sentiment values in new column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function to assign a polarity category to the sentiment\n",
    "def sentimentCategory(sent_num):\n",
    "    if sent_num >= 0.2:\n",
    "        return \"positive\"\n",
    "    if sent_num <= -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a new column to hold sentiment category\n",
    "df['sentiment_category'] = df['review_sentiment'].apply(sentimentCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewContent</th>\n",
       "      <th>review_sentiment</th>\n",
       "      <th>sentiment_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good. It IS a page turner. You can read this b...</td>\n",
       "      <td>-0.4086</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are no words for how much I loathed this...</td>\n",
       "      <td>-0.7432</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think I would ordinarily cut this book more ...</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three disjointed characters for whom it's hard...</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was snookered into this novel as it was compar...</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ReviewContent  review_sentiment  \\\n",
       "0  Good. It IS a page turner. You can read this b...           -0.4086   \n",
       "1  There are no words for how much I loathed this...           -0.7432   \n",
       "2  I think I would ordinarily cut this book more ...            0.9743   \n",
       "3  Three disjointed characters for whom it's hard...            0.5496   \n",
       "4  Was snookered into this novel as it was compar...            0.7329   \n",
       "\n",
       "  sentiment_category  \n",
       "0           negative  \n",
       "1           negative  \n",
       "2           positive  \n",
       "3           positive  \n",
       "4           positive  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good. It IS a page turner. You can read this book in one day, two at the most, and the plot drives the whole book. The unreliable narrators (there are two besides the main character) are as unlikable as they are unreliable, and there isn't a nice male in the book. Entirely plot driven; the characters are paper thin. You can figure out who-dunnit by the middle of the book. The ending is weak. I can't imagine what all the fuss is about, except that it is quick and there are lots of twists and turns, and you can't trust anyone to tell the truth.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ReviewContent'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Was snookered into this novel as it was compared to Gone Girl, which I liked a lot...This one is filled with dysfunctional characters who you wouldn\\'t care to spend any time with in real life...I figured out who \"done it\" early on and by the end I imagined an alternative ending which was not a happy ending for the whole lot of them...Can\\'t believe it remains on the bestseller list!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ReviewContent'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    3466\n",
       "negative    1013\n",
       "neutral      521\n",
       "Name: sentiment_category, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare frequency of positive, negative, and neutral reviews\n",
    "df['sentiment_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems that most readers feel so-so about the book (maybe some good parts and some bad parts) and some readers really like the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
